{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - TF-IDF Classifier\n",
    "\n",
    "Ваша цель обучить классификатор который будет находить \"токсичные\" комментарии и опубликовать решения на Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "В процессе обучения нужно ответить на ***[вопросы](https://docs.google.com/forms/d/e/1FAIpQLSd9mQx8EFpSH6FhCy1M_FmISzy3lhgyyqV3TN0pmtop7slmTA/viewform?usp=sf_link)***\n",
    "\n",
    "Данные можно скачать тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('../../data/Toxis comments/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../../data/Toxis comments/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стадартными подходами для анализа текста являются [Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) и его модификация [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Они реалзованны в `sklearn` в виде [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) и [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Более подробней про них можно посмотреть [тут](https://github.com/udsclub/workshop/blob/master/notebooks/UDS-workshop-feature-extraction-and-engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the most common words in the comments you should set max_features=1000 to avoid MemoryError\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  max_features=1000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_features = word_vectorizer.fit_transform(all_text)\n",
    "x_train = word_vectorizer.transform(train_text)\n",
    "x_test = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What words are the most common in the dataset\n",
    "answer depends on what Vectorizer we have chosen. But the most common is the same: 'the'\n",
    "### First method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = all_word_features.todense().sum(axis=0)\n",
    "d = list(zip(word_vectorizer.get_feature_names(), sums.tolist()[0]))\n",
    "words_sorted = sorted(d, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 35525.038806166674),\n",
       " ('to', 23633.798627869706),\n",
       " ('you', 22468.364330022436),\n",
       " ('of', 19213.909587014077),\n",
       " ('and', 18905.674429729585)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"return list of the words\"\"\"\n",
    "    pattern = r'[a-z]+'\n",
    "    words = re.findall(pattern, text.lower())\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lists = all_text.apply(get_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = dict()\n",
    "\n",
    "for item in words_lists:\n",
    "    \n",
    "    for word in list(item):\n",
    "        if word in all_words:\n",
    "            all_words[word] += 1\n",
    "        else:\n",
    "            all_words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 919075), ('to', 539244), ('i', 434390), ('a', 412567), ('of', 410841)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_words.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 919075), ('to', 539244), ('i', 434390), ('a', 412567), ('of', 410841)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Counter(all_words)\n",
    "result.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать логистическую регрессию [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем тренировать по одному классификатору на каждый класс. \n",
    "\n",
    "Что бы провалидировать качество модели воспользуемся функцией [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add some features to train and test datasets\n",
    "*don't make our predictions better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_more_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_more_features:\n",
    "    # TRAIN\n",
    "\n",
    "    train['count_word']=train[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    #Unique word count\n",
    "    train['count_unique_word']=train[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    #punctuation count\n",
    "    train[\"count_punctuations\"] =train[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in punctuation]))\n",
    "    #upper case words count\n",
    "    train[\"count_words_upper\"] = train[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    #Number of stopwords\n",
    "    train[\"count_stopwords\"] = train[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    #Average length of the words\n",
    "    train[\"mean_word_len\"] = train[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "\n",
    "    #TEST\n",
    "\n",
    "    test['count_word']=test[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    #Unique word count\n",
    "    test['count_unique_word']=test[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    #punctuation count\n",
    "    test[\"count_punctuations\"] =test[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in punctuation]))\n",
    "    #upper case words count\n",
    "    test[\"count_words_upper\"] = test[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    #Number of stopwords\n",
    "    test[\"count_stopwords\"] = test[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    #Average length of the words\n",
    "    test[\"mean_word_len\"] = test[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_more_features:\n",
    "    # create csr_matrixs\n",
    "    features_train = csr_matrix(train[train.columns[-6:]].fillna(0))\n",
    "    features_test = csr_matrix(test[test.columns[-6:]].fillna(0))\n",
    "\n",
    "    # concatanate with the train/test_words_features\n",
    "    x_train = hstack([x_train, features_train])\n",
    "    x_test = hstack([x_test, features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bf54af9e053a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_features' is not defined"
     ]
    }
   ],
   "source": [
    "scores= [] # best c_value for current class_name\n",
    "c_values = [0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 7.5, 10.0]\n",
    "\n",
    "\n",
    "for class_name in class_names:\n",
    "    \n",
    "    c_list = []\n",
    "    print('CLASS NAME: {}'.format(class_name))\n",
    "    \n",
    "    for c in c_values:\n",
    "        classifier = LogisticRegression(C=c, random_state=32)\n",
    "        \n",
    "        y_train = train[class_name]\n",
    "        cv_score = np.mean(cross_val_score(classifier, x_train, y_train, scoring='roc_auc'))\n",
    "    \n",
    "        print('CV score for c = {} is {}'.format(c, cv_score))\n",
    "        \n",
    "        c_list.append((c, cv_score))\n",
    "        \n",
    "    scores.append(max(c_list, key=lambda x: x[1]))\n",
    "    \n",
    "    print('-' * 20)\n",
    "\n",
    "sc = [item[1] for item in scores]\n",
    "print('Total score is {}'.format(np.mean(sc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photos of the previous results\n",
    "### 1\n",
    "![Photos of the previous results](http://localhost:8888/notebooks/pictures/Toxic_comments1.jpg)\n",
    "![Photos of the previous results](http://localhost:8888/notebooks/pictures/Toxic_comments2.jpg)\n",
    "### 2\n",
    "![Photos of the previous results](http://localhost:8888/notebooks/pictures/Toxic_comments3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте подобрать лучшие параметры для `word_vectorizer` и `classifier` оптимизируя метрику [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опубликуйте лучшие решение на [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [item[0] for item in scores]\n",
    "\n",
    "for class_name, c in zip(class_names, c_values):\n",
    "    \n",
    "    classifier = LogisticRegression(C=c, random_state=32)\n",
    "    \n",
    "    y_train = train[class_name]\n",
    "    classifier.fit(x_train, y_train)\n",
    "    \n",
    "    submission[class_name] = classifier.predict_proba(x_test)[:, 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "0  00001cee341fdb12\n",
       "1  0000247867823ef7\n",
       "2  00013b17ad220c46\n",
       "3  00017563c3f7919a\n",
       "4  00017695ad8997eb"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
